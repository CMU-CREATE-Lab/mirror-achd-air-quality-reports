{
 "metadata": {
  "name": "",
  "signature": "sha256:519bb8349cf87e37324eb468e876e80b8a7227d5e041d6e3ad77812e5ef475f1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parse ACHD\n",
      "==========\n",
      "\n",
      "Convert PDF reports to JSON-formatted data for Fluxtream upload"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime, fcntl, glob, json, math, os, re, subprocess, time, xml.dom.minidom\n",
      "from dateutil import tz\n",
      "\n",
      "force_reprocess = True\n",
      "enable_pdf_to_xml_cache = True\n",
      "enable_parse = False\n",
      "enable_upload = False\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "pdf2txt\n",
      "-------\n",
      "\n",
      "If you don't have pdf2txt or pdf2txt.py in your path, you'll need to install.\n",
      "See below for suggestions how to do so."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdf2txt_candidates = ['pdf2txt', 'pdf2txt.py']\n",
      "pdf2txt_path = None\n",
      "\n",
      "for candidate in pdf2txt_candidates:\n",
      "    try:\n",
      "        pdf2txt_path = subprocess.check_output(['which', candidate]).rstrip()\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "if not pdf2txt_path:\n",
      "    msg = 'Cannot find any of %s.\\n' % pdf2txt_candidates\n",
      "    msg += 'Please install pdf2txt.\\n'\n",
      "    msg += 'Ubuntu: sudo apt-get install python-pdfminer\\n'\n",
      "    msg += 'Mac: sudo port install py27-pdfminer\\n'\n",
      "    raise Exception(msg)\n",
      "    \n",
      "print 'Using pdf2txt from %s' % pdf2txt_path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using pdf2txt from /Users/rsargent/anaconda/bin/pdf2txt.py\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Timezone\n",
      "--------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "achd_tz = tz.tzoffset(\"EST\", -5 * 3600)\n",
      "\n",
      "# Allegheny health department always reports in Eastern Standard Time,\n",
      "# even when Pittsburgh observes Eastern Daylight Time during the summer.\n",
      "\n",
      "# In other words, ACHD times look correct in the winter, but are appear\n",
      "# to be one hour behind people's clocks during the summer.\n",
      "\n",
      "# For example, if during the summer, if\n",
      "# ACHD reports 3pm Eastern Standard Time,\n",
      "# that would correspond to 4pm Eastern Daylight Time.\n",
      "\n",
      "# Test that timezone has offset 5 hours (EST) both during summer and winter\n",
      "\n",
      "# Test this timezone.  Confirm epoch time of midnight 1/1/70 was 5 hours\n",
      "date = datetime.datetime.strptime('1/1/1970 00:00', '%m/%d/%Y %H:%M').replace(tzinfo=achd_tz)\n",
      "epoch = (date - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
      "if epoch != 5 * 3600:\n",
      "    raise Exception(\"Error in timezone\")\n",
      "\n",
      "date = datetime.datetime.strptime('7/1/1970 00:00', '%m/%d/%Y %H:%M').replace(tzinfo=achd_tz)\n",
      "epoch = (date - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
      "if epoch % 86400 != 5 * 3600:\n",
      "    raise Exception(\"Error in timezone\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Point:\n",
      "    def __init__(self, x, y):\n",
      "        self.x = x\n",
      "        self.y = y\n",
      "        \n",
      "    def __sub__(self, rhs):\n",
      "        return Point(self.x - rhs.x, self.y - rhs.y)\n",
      "        \n",
      "    def __repr__(self):\n",
      "        return 'Point2(%g, %g)' % (self.x, self.y)\n",
      "\n",
      "class Bbox:\n",
      "    def __init__(self, str):\n",
      "        (self.left, self.top, self.right, self.bottom) = [float(e) for e in str.split(',')]\n",
      "        \n",
      "    def __repr__(self):\n",
      "        return 'Bbox(%g, %g, %g, %g)' % (self.top, self.left, self.bottom, self.right)\n",
      "    \n",
      "    def center(self):\n",
      "        return Point(0.5 * (self.left + self.right), 0.5 * (self.top + self.bottom))\n",
      "    \n",
      "    def width(self):\n",
      "        return self.right - self.left\n",
      "    \n",
      "    def height(self):\n",
      "        return self.bottom - self.top"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log_lines = []\n",
      "\n",
      "def reset_log():\n",
      "    global log_lines\n",
      "    log_lines = []\n",
      "    \n",
      "def log(msg):\n",
      "    global log_lines\n",
      "    print msg\n",
      "    log_lines.append(datetime.datetime.now(tz.tzlocal()).strftime('%Y-%m-%d %H:%M:%S%z') + ' ' + msg)\n",
      "    \n",
      "def get_log():\n",
      "    return '\\n'.join(log_lines)\n",
      "    \n",
      "def epoch_time(dt):\n",
      "    epoch = datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())\n",
      "    return (dt - epoch).total_seconds()    \n",
      "\n",
      "def sanitize_name(name):\n",
      "    return re.sub('\\W', '_', name.encode('utf8'))\n",
      "\n",
      "\n",
      "def process_achd_site(achd_site, base_datetime, achd_table):\n",
      "    global global_achd_site\n",
      "    global global_base_datetime\n",
      "    global global_achd_table\n",
      "    global_achd_site = achd_site\n",
      "    global_base_datetime = base_datetime\n",
      "    global_achd_table = achd_table\n",
      "    # Replace non-alphanum with underscores in achd_site\n",
      "    devname = \"ACHD2_%s\" % (re.sub('\\W', '_',achd_site))\n",
      "    channel_names = []\n",
      "\n",
      "    # Process the header into channel names\n",
      "    for i in range (1, len(achd_table[0])):\n",
      "        channel_names.append(sanitize_name('%s_%s' % (achd_table[0][i], achd_table[1][i])))\n",
      "    \n",
      "    log(\"Found devname=%s, channels=%s, starting %s\" % (devname, channel_names, base_datetime))\n",
      "    \n",
      "    data = []\n",
      "\n",
      "    for i in range (2, len(achd_table)):\n",
      "        row = achd_table[i]\n",
      "        \n",
      "        # Add base_datetime to the first column of the row, which is the local time within that date\n",
      "        local_row_hour =  datetime.datetime.strptime(row[0], '%H:%M')\n",
      "        local_row_datetime = base_datetime.replace(hour=local_row_hour.hour).replace(tzinfo=achd_tz)\n",
      "        unix_ts = epoch_time(local_row_datetime)\n",
      "        \n",
      "        # data_row starts with epoch time\n",
      "        data_row = [unix_ts]\n",
      "        \n",
      "        # Add all samples to data_row.  Add false for missing or unparsable data\n",
      "        for j in range (1, len(row)):\n",
      "            try:\n",
      "                val_str = row[j].encode('utf8')\n",
      "                if \" \" in val_str:\n",
      "                    # condensation\n",
      "                    val_elts = val_str.split(' ')\n",
      "                    val_str = val_elts[0]\n",
      "                    annotation = val_elts[1]\n",
      "                    #print \"Ignoring annotation %s on %s, using %s [%d][%d]\" % (annotation, row[j], val_str, i, j)\n",
      "                data_row.append(float(val_str))\n",
      "            except:\n",
      "                data_row.append(False)\n",
      "        \n",
      "        data.append(data_row)\n",
      "    if enable_upload:\n",
      "        #fluxtream_upload(devname, channel_names, data)\n",
      "        log(\"Uploaded to devname=%s, channels=%s, starting %s\" % \n",
      "            (devname, channel_names, base_datetime))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#process_achd_site(global_achd_site, global_base_datetime, global_achd_table)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found devname=ACHD2_South_Fayette, channels=['INT_T_DEGC', 'OUT_T_DEGC', 'OZONE_PPM', 'OZONE2_PPM', 'SIGTHETA_DEG', 'SO2_PPM', 'SONICWD_DEG', 'SONICWS_MPH'], starting 2014-10-11 00:00:00\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_textline(textline):\n",
      "    bbox = Bbox(textline.getAttribute('bbox'))\n",
      "    text = ''.join([elt.firstChild.nodeValue for elt in textline.getElementsByTagName('text')])\n",
      "    return {'bbox':bbox, 'text': text}\n",
      "\n",
      "def process_page(page):\n",
      "    textlines = [parse_textline(textline) for textline in page.getElementsByTagName('textline')]\n",
      "\n",
      "    # Find site\n",
      "    site = None\n",
      "\n",
      "    for (i, textline) in enumerate(textlines):\n",
      "        if textline['text'].strip() == 'Site:':\n",
      "            site_textline = textlines[i + 1]\n",
      "            deltapos = textline['bbox'].center() - site_textline['bbox'].center()\n",
      "        \n",
      "            if abs(deltapos.y) > 3:\n",
      "                raise Exception('Confused about y location while trying to locate site')\n",
      "        \n",
      "            if deltapos.x > 0 or deltapos.x < -100:\n",
      "                raise Exception('Confused about x location while trying to locate site')\n",
      "        \n",
      "            if site:\n",
      "                raise Exception('Found more than one site?')\n",
      "        \n",
      "            site = site_textline['text'].strip()\n",
      "\n",
      "    if not site:\n",
      "        raise Exception(\"Couldn't parse site\")\n",
      "\n",
      "    log('Site: %s' % site)\n",
      "\n",
      "    # Find date\n",
      "    date = None\n",
      "    for textline in textlines:\n",
      "        if re.match(r'\\d+/\\d+/\\d+$', textline['text'].strip()):\n",
      "            if date:\n",
      "                raise Exception('Found more than one date?')\n",
      "            date = datetime.datetime.strptime(textline['text'].strip(), '%m/%d/%Y')\n",
      "\n",
      "    if not date:\n",
      "        raise Exception(\"Couldn't find date\")\n",
      "\n",
      "    log('Date: %s' % date)\n",
      "\n",
      "    toprow = None\n",
      "    bottomrow = None\n",
      "    columns = []\n",
      "\n",
      "    # Find row locations\n",
      "    for textline in textlines:\n",
      "        if textline['text'].strip() == '00:00':\n",
      "            if toprow:\n",
      "                raise Exception('Found more than one toprow?')\n",
      "            toprow = textline['bbox'].center().y\n",
      "            columns.append(textline['bbox'].center().x)\n",
      "        if textline['text'].strip() == '23:00':\n",
      "            if bottomrow:\n",
      "                raise Exception('Found more than one bottomrow?')\n",
      "            bottomrow = textline['bbox'].center().y\n",
      "\n",
      "    if not toprow or not bottomrow:\n",
      "        raise Exception(\"Couldn't find row landmarks\")\n",
      "\n",
      "    def compute_row(textline):\n",
      "        fraction = (toprow - textline['bbox'].center().y) / float(toprow - bottomrow)\n",
      "        row = round((fraction * 23) + 2)\n",
      "        if (row < 0 or row > 25):\n",
      "            return None\n",
      "        return int(row)\n",
      "\n",
      "    # Find columns\n",
      "    for textline in textlines:\n",
      "        if 0 == compute_row(textline):\n",
      "            columns.append(textline['bbox'].center().x)\n",
      "\n",
      "    columns = sorted(columns)\n",
      "\n",
      "    def compute_col(textline):\n",
      "        x = textline['bbox'].center().x\n",
      "        best = 0\n",
      "        for i in range(0, len(columns)):\n",
      "            if abs(x - columns[i]) < abs(x - columns[best]):\n",
      "                best = i\n",
      "        return best\n",
      "\n",
      "    table = [[False] * len(columns) for row in range(0, 26)]\n",
      "\n",
      "    # Build table\n",
      "    # Find columns\n",
      "    for textline in textlines:\n",
      "        row = compute_row(textline)\n",
      "        if row != None:\n",
      "            col = compute_col(textline)\n",
      "            table[row][col] = textline['text'].strip()\n",
      "\n",
      "    process_achd_site(site, date, table)\n",
      "    \n",
      "def process_doc(doc):\n",
      "    for (pageno, page) in enumerate(doc.getElementsByTagName('page')):\n",
      "        log('-------------------------------------------------------------------------------')\n",
      "        log('Processing page %d' % pageno)\n",
      "        process_page(page)\n",
      "        log('-------------------------------------------------------------------------------')\n",
      "\n",
      "\n",
      "\n",
      "def pdf_to_xml(pdf_path):\n",
      "    cache_file = pdf_path + '.cache.xml'\n",
      "    if enable_pdf_to_xml_cache and os.path.exists(cache_file):\n",
      "        print 'Using %s from cache' % (cache_file)\n",
      "        return open(cache_file).read()\n",
      "    else:\n",
      "        log('Converting %s to xml' % pdf_path)\n",
      "        start_time = time.time()\n",
      "        xml_content = subprocess.check_output([pdf2txt_path, '-t', 'xml', pdf_path])\n",
      "        print 'pdf %s length %d converted to xml length %d in %.1f seconds' % (pdf_path, os.stat(pdf_path).st_size, len(xml_content), time.time() - start_time)\n",
      "        if enable_pdf_to_xml_cache:\n",
      "            tmp_file = '%s.tmp.%d' % (cache_file, os.getpid())\n",
      "            open(tmp_file, 'w').write(xml_content)\n",
      "            os.rename(tmp_file, cache_file)\n",
      "            print 'xml written to %s' % cache_file\n",
      "        return xml_content\n",
      "    \n",
      "def process_pdf(pdf_path):\n",
      "    done_dir = 'upload-to-fluxtream'\n",
      "    try:\n",
      "        os.mkdir(done_dir)\n",
      "    except OSError:\n",
      "        pass\n",
      "    done_path = done_dir + '/' + os.path.basename(os.path.splitext(pdf_path)[0]) + '.successful-upload'\n",
      "    if not force_reprocess and os.path.exists(done_path):\n",
      "        log('%s already uploaded, skipping' % pdf_path)\n",
      "        return\n",
      "    reset_log()\n",
      "    \n",
      "    xml_content = pdf_to_xml(pdf_path)\n",
      "    if not enable_parse:\n",
      "        print 'Parse disabled'\n",
      "        return\n",
      "    log('Parsing and uploading %s' % pdf_path)\n",
      "    start_time = time.time()\n",
      "    doc = xml.dom.minidom.parseString(xml_content)\n",
      "    process_doc(doc)\n",
      "    if enable_upload:\n",
      "        open(done_path + '.tmp', 'w').write(get_log())\n",
      "        os.rename(done_path + '.tmp', done_path)\n",
      "    print 'XML parsed and uploaded in %.1f seconds' % (time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#process_pdf('mirror/DailySummary-2014-10-12-00:35:02-0400.PDF')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using mirror/DailySummary-2014-10-12-00:35:02-0400.PDF.cache.xml from cache\n",
        "Parse disabled\n"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#global_achd_table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "[[False,\n",
        "  u'BP',\n",
        "  u'H2S',\n",
        "  u'INT_T',\n",
        "  u'OUT_T',\n",
        "  u'PM25B',\n",
        "  u'SIGTHETA',\n",
        "  u'SO2',\n",
        "  u'SONICWD',\n",
        "  u'SONICWS'],\n",
        " [u'Hour',\n",
        "  u'MM/HG',\n",
        "  u'PPM',\n",
        "  u'DEGC',\n",
        "  u'DEGC',\n",
        "  u'UG/M3',\n",
        "  u'DEG',\n",
        "  u'PPM',\n",
        "  u'DEG',\n",
        "  u'MPH'],\n",
        " [u'00:00',\n",
        "  u'743.0',\n",
        "  u'.000 C',\n",
        "  u'23.3',\n",
        "  u'8.2',\n",
        "  u'8',\n",
        "  u'24.6',\n",
        "  u'.000 C',\n",
        "  u'69',\n",
        "  u'1.0'],\n",
        " [u'01:00',\n",
        "  u'742.9',\n",
        "  u'.001 C',\n",
        "  u'23.1',\n",
        "  u'8.7',\n",
        "  u'8',\n",
        "  u'60.7 -',\n",
        "  u'.000 C',\n",
        "  u'22',\n",
        "  u'.5'],\n",
        " [u'02:00',\n",
        "  u'742.9',\n",
        "  u'.001',\n",
        "  u'23.2',\n",
        "  u'9.1',\n",
        "  u'10',\n",
        "  u'65.4 -',\n",
        "  u'.000',\n",
        "  u'280',\n",
        "  u'.6'],\n",
        " [u'03:00',\n",
        "  u'742.9',\n",
        "  u'.000',\n",
        "  u'23.3',\n",
        "  u'9.0',\n",
        "  u'11',\n",
        "  u'46.5 -',\n",
        "  u'.000',\n",
        "  u'349',\n",
        "  u'.6'],\n",
        " [u'04:00',\n",
        "  u'742.9',\n",
        "  u'.001',\n",
        "  u'23.4',\n",
        "  u'9.1',\n",
        "  u'19',\n",
        "  u'51.0 -',\n",
        "  u'.000',\n",
        "  u'61',\n",
        "  u'.4'],\n",
        " [u'05:00',\n",
        "  u'743.1',\n",
        "  u'.001',\n",
        "  u'23.4',\n",
        "  u'9.1',\n",
        "  u'19',\n",
        "  u'48.9',\n",
        "  u'.000',\n",
        "  u'91',\n",
        "  u'.9'],\n",
        " [u'06:00',\n",
        "  u'743.1',\n",
        "  u'.001',\n",
        "  u'23.4',\n",
        "  u'9.0',\n",
        "  u'21',\n",
        "  u'20.3',\n",
        "  u'.000',\n",
        "  u'92',\n",
        "  u'2.1'],\n",
        " [u'07:00',\n",
        "  u'743.2',\n",
        "  u'.000',\n",
        "  u'23.4',\n",
        "  u'8.8',\n",
        "  u'16',\n",
        "  u'32.7 -',\n",
        "  u'.000',\n",
        "  u'74',\n",
        "  u'1.7'],\n",
        " [u'08:00',\n",
        "  u'743.5',\n",
        "  u'.000',\n",
        "  u'23.5',\n",
        "  u'9.0',\n",
        "  u'9',\n",
        "  u'56.8 -',\n",
        "  u'.000',\n",
        "  u'45',\n",
        "  u'1.3'],\n",
        " [u'09:00',\n",
        "  u'743.5',\n",
        "  u'.000',\n",
        "  u'23.6',\n",
        "  u'9.5',\n",
        "  u'984 H',\n",
        "  u'52.2 -',\n",
        "  u'.000',\n",
        "  u'24',\n",
        "  u'1.4'],\n",
        " [u'10:00',\n",
        "  u'743.2',\n",
        "  u'D',\n",
        "  u'22.4',\n",
        "  u'10.4',\n",
        "  u'984 H',\n",
        "  u'71.4 -',\n",
        "  u'.000',\n",
        "  u'58',\n",
        "  u'1.3'],\n",
        " [u'11:00',\n",
        "  u'743.1',\n",
        "  u'.001 D',\n",
        "  u'22.7',\n",
        "  u'12.1',\n",
        "  u'8',\n",
        "  u'38.4',\n",
        "  u'D',\n",
        "  u'223',\n",
        "  u'2.4'],\n",
        " [u'12:00',\n",
        "  u'742.7',\n",
        "  u'.001',\n",
        "  u'23.8',\n",
        "  u'14.0',\n",
        "  u'19',\n",
        "  u'61.3 -',\n",
        "  u'.002',\n",
        "  u'251',\n",
        "  u'2.0'],\n",
        " [u'13:00',\n",
        "  u'742.4',\n",
        "  u'.000',\n",
        "  u'23.6',\n",
        "  u'15.1',\n",
        "  u'14',\n",
        "  u'68.9 -',\n",
        "  u'.001',\n",
        "  u'4',\n",
        "  u'2.4'],\n",
        " [u'14:00',\n",
        "  u'742.2',\n",
        "  u'.000',\n",
        "  u'23.7',\n",
        "  u'15.2',\n",
        "  u'12',\n",
        "  u'60.5 -',\n",
        "  u'.001',\n",
        "  u'350',\n",
        "  u'1.4'],\n",
        " [u'15:00',\n",
        "  u'742.0',\n",
        "  u'.000',\n",
        "  u'24.0',\n",
        "  u'15.0',\n",
        "  u'10',\n",
        "  u'60.9 -',\n",
        "  u'.000',\n",
        "  u'357',\n",
        "  u'1.1'],\n",
        " [u'16:00',\n",
        "  u'742.0',\n",
        "  u'.000',\n",
        "  u'24.2',\n",
        "  u'14.8',\n",
        "  u'16',\n",
        "  u'40.8 -',\n",
        "  u'.000',\n",
        "  u'334',\n",
        "  u'.9'],\n",
        " [u'17:00',\n",
        "  u'742.0',\n",
        "  u'.000',\n",
        "  u'24.4',\n",
        "  u'13.9',\n",
        "  u'13',\n",
        "  u'45.5 -',\n",
        "  u'.000',\n",
        "  u'12',\n",
        "  u'.9'],\n",
        " [u'18:00',\n",
        "  u'742.3',\n",
        "  u'.000',\n",
        "  u'24.4',\n",
        "  u'13.2',\n",
        "  u'20',\n",
        "  u'57.9 -',\n",
        "  u'.000',\n",
        "  u'343',\n",
        "  u'.7'],\n",
        " [u'19:00',\n",
        "  u'742.3',\n",
        "  u'.000',\n",
        "  u'24.6',\n",
        "  u'12.8',\n",
        "  u'13',\n",
        "  u'47.8 -',\n",
        "  u'.000',\n",
        "  u'350',\n",
        "  u'1.0'],\n",
        " [u'20:00',\n",
        "  u'742.5',\n",
        "  u'.000',\n",
        "  u'24.8',\n",
        "  u'12.5',\n",
        "  u'12',\n",
        "  u'71.5 -',\n",
        "  u'.000',\n",
        "  u'15',\n",
        "  u'.8'],\n",
        " [u'21:00',\n",
        "  u'742.3',\n",
        "  u'.000',\n",
        "  u'24.6',\n",
        "  u'12.1',\n",
        "  u'12',\n",
        "  u'83.1 -',\n",
        "  u'.000',\n",
        "  u'333',\n",
        "  u'1.2'],\n",
        " [u'22:00',\n",
        "  u'742.1',\n",
        "  u'.000',\n",
        "  u'24.8',\n",
        "  u'11.5',\n",
        "  u'13',\n",
        "  u'63.8 -',\n",
        "  u'.000',\n",
        "  u'16',\n",
        "  u'1.7'],\n",
        " [u'23:00',\n",
        "  u'742.1',\n",
        "  u'.000',\n",
        "  u'24.7',\n",
        "  u'11.1',\n",
        "  u'12',\n",
        "  u'49.0 -',\n",
        "  u'.000',\n",
        "  u'341',\n",
        "  u'1.7']]"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_all():\n",
      "    lockfile = open('parse-achd.lockfile','w')\n",
      "    try:\n",
      "        fcntl.flock(lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
      "    except IOError:\n",
      "        print 'Instance of %s is already running.  Exiting.' % __file__\n",
      "        return\n",
      "    for pdf in sorted(glob.glob('mirror/*.PDF')):\n",
      "        process_pdf(pdf)\n",
      "\n",
      "process_all()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "timemachine1:/usr4/web/data.cmucreatelab.org/mirror-achd-air-quality-reports/mirror\n",
      "\n",
      "There are around 180 individual days\n",
      "Individual day takes around 2 minutes\n",
      "\n",
      "180 days * 2 min/day = 360 mins = 6 hrs\n",
      "\n",
      "Plus 500 days of historical reports\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}