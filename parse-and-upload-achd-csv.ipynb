{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse And Upload ACHD\n",
    "\n",
    "Convert CSV reports (starting 2017) to JSON-formatted data for ESDR upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reprocessing of earlier uploaded files\n",
    "force_reprocess = False\n",
    "\n",
    "# Dry run -- everything but actual upload\n",
    "dry_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, datetime, fcntl, glob, json, math, os, re, subprocess, sys, time, xml.dom.minidom\n",
    "from dateutil import tz\n",
    "\n",
    "# To install dateutil on Ubuntu\n",
    "# sudo pip install python-dateutil\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (urllib2.urlopen(filename_or_url) if re.match(r'https?:', filename_or_url) else open(filename_or_url)).read()\n",
    "    jsonNb = json.loads(nb)\n",
    "    #check for the modified formatting of Jupyter Notebook v4\n",
    "    if(jsonNb['nbformat'] == 4):\n",
    "        exec '\\n'.join([''.join(cell['source']) for cell in jsonNb['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "    else:\n",
    "        exec '\\n'.join([''.join(cell['input']) for cell in jsonNb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "\n",
    "exec_ipynb('python-utils/esdr-library.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timezone\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "achd_tz = tz.tzoffset(\"EST\", -5 * 3600)\n",
    "\n",
    "# Allegheny health department always reports in Eastern Standard Time,\n",
    "# even when Pittsburgh observes Eastern Daylight Time during the summer.\n",
    "\n",
    "# In other words, ACHD times look correct in the winter, but are appear\n",
    "# to be one hour behind people's clocks during the summer.\n",
    "\n",
    "# For example, if during the summer, if\n",
    "# ACHD reports 3pm Eastern Standard Time,\n",
    "# that would correspond to 4pm Eastern Daylight Time.\n",
    "\n",
    "# Test that timezone has offset 5 hours (EST) both during summer and winter\n",
    "\n",
    "# Test this timezone.  Confirm epoch time of midnight 1/1/70 was 5 hours\n",
    "date = datetime.datetime.strptime('1/1/1970 00:00', '%m/%d/%Y %H:%M').replace(tzinfo=achd_tz)\n",
    "epoch = (date - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
    "if epoch != 5 * 3600:\n",
    "    raise Exception(\"Error in timezone\")\n",
    "\n",
    "date = datetime.datetime.strptime('7/1/1970 00:00', '%m/%d/%Y %H:%M').replace(tzinfo=achd_tz)\n",
    "epoch = (date - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
    "if epoch % 86400 != 5 * 3600:\n",
    "    raise Exception(\"Error in timezone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and upload single CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esdr = None\n",
    "esdr_product = None\n",
    "\n",
    "def epoch_time(dt):\n",
    "    epoch = datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())\n",
    "    return (dt - epoch).total_seconds()    \n",
    "\n",
    "# Process a table parsed from a single day\n",
    "# Should have 26 total rows\n",
    "# First two rows are column headers\n",
    "# Next 24 are the hourly data\n",
    "\n",
    "def process_achd_site(achd_site, base_datetime, achd_table):\n",
    "    global global_achd_site\n",
    "    global global_base_datetime\n",
    "    global global_achd_table\n",
    "    global_achd_site = achd_site\n",
    "    global_base_datetime = base_datetime\n",
    "    global_achd_table = achd_table\n",
    "    # Replace non-alphanum with underscores in achd_site\n",
    "    devname = \"%s\" % (re.sub('\\W+', ' ',achd_site))\n",
    "    channel_names = []\n",
    "\n",
    "    # Process the header into channel names\n",
    "    for i in range (1, len(achd_table[0])):\n",
    "        if achd_table[0][i] == '':\n",
    "            break\n",
    "        channel_names.append(Esdr.make_identifier('%s_%s' % (achd_table[0][i], achd_table[1][i])))\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for i in range (2, len(achd_table)):\n",
    "        row = achd_table[i]\n",
    "        \n",
    "        # Add base_datetime to the first column of the row, which is the local time within that date\n",
    "        local_row_hour =  datetime.datetime.strptime(row[0], '%H:%M')\n",
    "        local_row_datetime = base_datetime.replace(hour=local_row_hour.hour).replace(tzinfo=achd_tz)\n",
    "        # Offset time by 1800 seconds so we're at the center of the hour-long sample\n",
    "        unix_ts = epoch_time(local_row_datetime) + 1800\n",
    "        \n",
    "        # data_row starts with epoch time\n",
    "        data_row = [unix_ts]\n",
    "        \n",
    "        # Add all samples to data_row.  Add false for missing or unparsable data\n",
    "        for j in range (1, len(channel_names) + 1):\n",
    "            try:\n",
    "                val_str = row[j].encode('utf8')\n",
    "                if \" \" in val_str:\n",
    "                    # condensation\n",
    "                    val_elts = val_str.split(' ')\n",
    "                    val_str = val_elts[0]\n",
    "                    annotation = val_elts[1]\n",
    "                    #print \"Ignoring annotation %s on %s, using %s [%d][%d]\" % (annotation, row[j], val_str, i, j)\n",
    "                data_row.append(float(val_str))\n",
    "            except:\n",
    "                data_row.append(False)\n",
    "        \n",
    "        data.append(data_row)\n",
    "    if dry_run:\n",
    "        print (\"Would have uploaded to devname=%s, channels=%s, starting %s\" %\n",
    "               (devname, channel_names, base_datetime))\n",
    "    else:\n",
    "        global esdr, esdr_product\n",
    "        if not esdr:\n",
    "            esdr = Esdr('esdr-auth.json')\n",
    "        esdr_product = esdr.get_product_by_name('ACHD')\n",
    "        serial_number = re.sub(r'\\s+', '_', devname)\n",
    "        device = esdr.get_or_create_device(esdr_product, serial_number, name=devname)\n",
    "        feed = esdr.get_or_create_feed(device)\n",
    "        before = time.time()\n",
    "        esdr.upload(feed, {\n",
    "            'channel_names': channel_names,\n",
    "            'data': data\n",
    "        });\n",
    "        \n",
    "        \n",
    "        print (\"Uploaded to devname=%s, channels=%s, starting %s.  Upload took %.1f seconds\" % \n",
    "            (devname, channel_names, base_datetime, time.time() - before))\n",
    "        \n",
    "def process_page(page):\n",
    "    # Find site name (e.g. 'Avalon')\n",
    "    assert page[2][0] == 'Site:'\n",
    "    site = page[2][1]\n",
    "    \n",
    "    # Find date for data from page (e.g. '4/18/2017')\n",
    "    date = datetime.datetime.strptime(page[2][5].strip(), '%m/%d/%Y')\n",
    "\n",
    "    assert page[5][0] == '00:00'\n",
    "    assert page[28][0] == '23:00'\n",
    "    \n",
    "    process_achd_site(site, date, page[3:29])\n",
    "\n",
    "def process_csv_file(path):\n",
    "    page = []\n",
    "    for row in csv.reader(open(path)):\n",
    "        if re.match(r'Date Printed', row[0]):\n",
    "            # Start new page\n",
    "            if len(page):\n",
    "                process_page(page)\n",
    "                page = []\n",
    "        page.append(row)\n",
    "    if len(page):\n",
    "        process_page(page)\n",
    "    \n",
    "# process_csv_file('mirror-csv/AirQualityDataSummary-2017-05-25-historical-report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find new CSV files not yet parsed and uploaded, and parse and upload them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_done_path(csv_path):\n",
    "    done_dir = 'upload-to-esdr'\n",
    "    try:\n",
    "        os.mkdir(done_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    return done_dir + '/' + os.path.basename(os.path.splitext(csv_path)[0]) + '.successful-upload'\n",
    "\n",
    "def process_all():\n",
    "    count = 0\n",
    "    for csv in sorted(glob.glob('mirror-csv/*.csv')):\n",
    "        done_file = compute_done_path(csv)\n",
    "        if not os.path.exists(done_file):\n",
    "            print 'Processing %s...' % csv\n",
    "            process_csv_file(csv)\n",
    "            open(done_file, 'w') # create empty file\n",
    "            count += 1\n",
    "    if count:\n",
    "        print 'Parsed and uploaded %d CSV files' % count\n",
    "    else:\n",
    "        print 'No new CSV files to parse and upload, exiting'\n",
    "        \n",
    "process_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
