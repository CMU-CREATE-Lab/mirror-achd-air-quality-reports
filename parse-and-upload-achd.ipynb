{
 "metadata": {
  "name": "",
  "signature": "sha256:afaf3b77654af08947e02f5ef9b9d59b80743aa2f20b4b9f2880eb37a6213c4e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parse And Upload ACHD\n",
      "=====================\n",
      "\n",
      "Convert PDF reports to JSON-formatted data for ESDR upload"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime, fcntl, glob, json, math, os, re, subprocess, sys, time, xml.dom.minidom\n",
      "from dateutil import tz\n",
      "\n",
      "# To install dateutil on Ubuntu\n",
      "# sudo pip install python-dateutil\n",
      "\n",
      "def exec_ipynb(url):\n",
      "    import json, re, urllib2\n",
      "    nb = (urllib2.urlopen(url) if re.match(r'https?:', url) else open(url)).read()\n",
      "    exec '\\n'.join([''.join(cell['input']) for cell in json.loads(nb)['worksheets'][0]['cells'] if cell['cell_type'] == 'code']) in globals()\n",
      "\n",
      "exec_ipynb('esdr-library.ipynb')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Force reprocessing of earlier uploaded files\n",
      "force_reprocess = False\n",
      "\n",
      "# Cache PDF to XML, in case of future reprocessing\n",
      "enable_pdf_to_xml_cache = False\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "pdf2txt\n",
      "-------\n",
      "\n",
      "If you don't have pdf2txt or pdf2txt.py in your path, you'll need to install.\n",
      "See below for suggestions how to do so."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdf2txt_candidates = ['pdf2txt', 'pdf2txt.py']\n",
      "pdf2txt_path = None\n",
      "\n",
      "for candidate in pdf2txt_candidates:\n",
      "    try:\n",
      "        pdf2txt_path = subprocess.check_output(['which', candidate]).rstrip()\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "if not pdf2txt_path:\n",
      "    msg = 'Cannot find any of %s.\\n' % pdf2txt_candidates\n",
      "    msg += 'Please install pdf2txt.\\n'\n",
      "    msg += 'Ubuntu: sudo apt-get install python-pdfminer\\n'\n",
      "    msg += 'Mac: sudo port install py27-pdfminer\\n'\n",
      "    raise Exception(msg)\n",
      "    \n",
      "print 'Using pdf2txt from %s' % pdf2txt_path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using pdf2txt from /Users/rsargent/anaconda/bin/pdf2txt.py\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Timezone\n",
      "--------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "achd_tz = tz.tzoffset(\"EST\", -5 * 3600)\n",
      "\n",
      "# Allegheny health department always reports in Eastern Standard Time,\n",
      "# even when Pittsburgh observes Eastern Daylight Time during the summer.\n",
      "\n",
      "# In other words, ACHD times look correct in the winter, but are appear\n",
      "# to be one hour behind people's clocks during the summer.\n",
      "\n",
      "# For example, if during the summer, if\n",
      "# ACHD reports 3pm Eastern Standard Time,\n",
      "# that would correspond to 4pm Eastern Daylight Time.\n",
      "\n",
      "# Test that timezone has offset 5 hours (EST) both during summer and winter\n",
      "\n",
      "# Test this timezone.  Confirm epoch time of midnight 1/1/70 was 5 hours\n",
      "date = datetime.datetime.strptime('1/1/1970 00:00', '%m/%d/%Y %H:%M').replace(tzinfo=achd_tz)\n",
      "epoch = (date - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
      "if epoch != 5 * 3600:\n",
      "    raise Exception(\"Error in timezone\")\n",
      "\n",
      "date = datetime.datetime.strptime('7/1/1970 00:00', '%m/%d/%Y %H:%M').replace(tzinfo=achd_tz)\n",
      "epoch = (date - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
      "if epoch % 86400 != 5 * 3600:\n",
      "    raise Exception(\"Error in timezone\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Point:\n",
      "    def __init__(self, x, y):\n",
      "        self.x = x\n",
      "        self.y = y\n",
      "        \n",
      "    def __sub__(self, rhs):\n",
      "        return Point(self.x - rhs.x, self.y - rhs.y)\n",
      "        \n",
      "    def __repr__(self):\n",
      "        return 'Point2(%g, %g)' % (self.x, self.y)\n",
      "\n",
      "class Bbox:\n",
      "    def __init__(self, str):\n",
      "        (self.left, self.top, self.right, self.bottom) = [float(e) for e in str.split(',')]\n",
      "        \n",
      "    def __repr__(self):\n",
      "        return 'Bbox(%g, %g, %g, %g)' % (self.top, self.left, self.bottom, self.right)\n",
      "    \n",
      "    def center(self):\n",
      "        return Point(0.5 * (self.left + self.right), 0.5 * (self.top + self.bottom))\n",
      "    \n",
      "    def width(self):\n",
      "        return self.right - self.left\n",
      "    \n",
      "    def height(self):\n",
      "        return self.bottom - self.top"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enable parsing PDF\n",
      "enable_parse = True\n",
      "\n",
      "# Enable upload of PDF\n",
      "enable_upload = True\n",
      "\n",
      "log_lines = []\n",
      "\n",
      "def reset_log():\n",
      "    global log_lines\n",
      "    log_lines = []\n",
      "    \n",
      "def log(msg):\n",
      "    global log_lines\n",
      "    print msg\n",
      "    log_lines.append(datetime.datetime.now(tz.tzlocal()).strftime('%Y-%m-%d %H:%M:%S%z') + ' ' + msg)\n",
      "    \n",
      "def get_log():\n",
      "    return '\\n'.join(log_lines)\n",
      "    \n",
      "def epoch_time(dt):\n",
      "    epoch = datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())\n",
      "    return (dt - epoch).total_seconds()    \n",
      "\n",
      "def sanitize_name(name):\n",
      "    return re.sub('\\W', '_', name.encode('utf8'))\n",
      "\n",
      "\n",
      "def process_achd_site(achd_site, base_datetime, achd_table):\n",
      "    global global_achd_site\n",
      "    global global_base_datetime\n",
      "    global global_achd_table\n",
      "    global_achd_site = achd_site\n",
      "    global_base_datetime = base_datetime\n",
      "    global_achd_table = achd_table\n",
      "    # Replace non-alphanum with underscores in achd_site\n",
      "    devname = \"%s\" % (re.sub('\\W+', ' ',achd_site))\n",
      "    channel_names = []\n",
      "\n",
      "    # Process the header into channel names\n",
      "    for i in range (1, len(achd_table[0])):\n",
      "        channel_names.append(sanitize_name('%s_%s' % (achd_table[0][i], achd_table[1][i])))\n",
      "    \n",
      "    data = []\n",
      "\n",
      "    for i in range (2, len(achd_table)):\n",
      "        row = achd_table[i]\n",
      "        \n",
      "        # Add base_datetime to the first column of the row, which is the local time within that date\n",
      "        local_row_hour =  datetime.datetime.strptime(row[0], '%H:%M')\n",
      "        local_row_datetime = base_datetime.replace(hour=local_row_hour.hour).replace(tzinfo=achd_tz)\n",
      "        # Offset time by 1800 seconds so we're at the center of the hour-long sample\n",
      "        unix_ts = epoch_time(local_row_datetime) + 1800\n",
      "        \n",
      "        # data_row starts with epoch time\n",
      "        data_row = [unix_ts]\n",
      "        \n",
      "        # Add all samples to data_row.  Add false for missing or unparsable data\n",
      "        for j in range (1, len(row)):\n",
      "            try:\n",
      "                val_str = row[j].encode('utf8')\n",
      "                if \" \" in val_str:\n",
      "                    # condensation\n",
      "                    val_elts = val_str.split(' ')\n",
      "                    val_str = val_elts[0]\n",
      "                    annotation = val_elts[1]\n",
      "                    #print \"Ignoring annotation %s on %s, using %s [%d][%d]\" % (annotation, row[j], val_str, i, j)\n",
      "                data_row.append(float(val_str))\n",
      "            except:\n",
      "                data_row.append(False)\n",
      "        \n",
      "        data.append(data_row)\n",
      "    if enable_upload:\n",
      "        device = esdr.get_or_create_device(esdr_product, devname)\n",
      "        feed = esdr.get_or_create_feed(device)\n",
      "        before = time.time()\n",
      "        esdr.upload(feed, {\n",
      "            'channel_names': channel_names,\n",
      "            'data': data\n",
      "        });\n",
      "        \n",
      "        \n",
      "        log(\"Uploaded to devname=%s, channels=%s, starting %s.  Upload took %.1f seconds\" % \n",
      "            (devname, channel_names, base_datetime, time.time() - before))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_textline(textline):\n",
      "    bbox = Bbox(textline.getAttribute('bbox'))\n",
      "    text = ''.join([elt.firstChild.nodeValue for elt in textline.getElementsByTagName('text')])\n",
      "    return {'bbox':bbox, 'text': text}\n",
      "\n",
      "def process_page(page):\n",
      "    textlines = [parse_textline(textline) for textline in page.getElementsByTagName('textline')]\n",
      "\n",
      "    # Find site\n",
      "    site = None\n",
      "\n",
      "    for (i, textline) in enumerate(textlines):\n",
      "        if textline['text'].strip() == 'Site:':\n",
      "            site_textline = textlines[i + 1]\n",
      "            deltapos = textline['bbox'].center() - site_textline['bbox'].center()\n",
      "        \n",
      "            if abs(deltapos.y) > 3:\n",
      "                raise Exception('Confused about y location while trying to locate site')\n",
      "        \n",
      "            if deltapos.x > 0 or deltapos.x < -100:\n",
      "                raise Exception('Confused about x location while trying to locate site')\n",
      "        \n",
      "            if site:\n",
      "                raise Exception('Found more than one site?')\n",
      "        \n",
      "            site = site_textline['text'].strip()\n",
      "\n",
      "    if not site:\n",
      "        raise Exception(\"Couldn't parse site\")\n",
      "\n",
      "    # Find date\n",
      "    date = None\n",
      "    for textline in textlines:\n",
      "        if re.match(r'\\d+/\\d+/\\d+$', textline['text'].strip()):\n",
      "            if date:\n",
      "                raise Exception('Found more than one date?')\n",
      "            date = datetime.datetime.strptime(textline['text'].strip(), '%m/%d/%Y')\n",
      "\n",
      "    if not date:\n",
      "        raise Exception(\"Couldn't find date\")\n",
      "\n",
      "    toprow = None\n",
      "    bottomrow = None\n",
      "    columns = []\n",
      "\n",
      "    # Find row locations\n",
      "    for textline in textlines:\n",
      "        if textline['text'].strip() == '00:00':\n",
      "            if toprow:\n",
      "                raise Exception('Found more than one toprow?')\n",
      "            toprow = textline['bbox'].center().y\n",
      "            columns.append(textline['bbox'].center().x)\n",
      "        if textline['text'].strip() == '23:00':\n",
      "            if bottomrow:\n",
      "                raise Exception('Found more than one bottomrow?')\n",
      "            bottomrow = textline['bbox'].center().y\n",
      "\n",
      "    if not toprow or not bottomrow:\n",
      "        raise Exception(\"Couldn't find row landmarks\")\n",
      "\n",
      "    def compute_row(textline):\n",
      "        fraction = (toprow - textline['bbox'].center().y) / float(toprow - bottomrow)\n",
      "        row = round((fraction * 23) + 2)\n",
      "        if (row < 0 or row > 25):\n",
      "            return None\n",
      "        return int(row)\n",
      "\n",
      "    # Find columns\n",
      "    for textline in textlines:\n",
      "        if 0 == compute_row(textline):\n",
      "            columns.append(textline['bbox'].center().x)\n",
      "\n",
      "    columns = sorted(columns)\n",
      "\n",
      "    def compute_col(textline):\n",
      "        x = textline['bbox'].center().x\n",
      "        best = 0\n",
      "        for i in range(0, len(columns)):\n",
      "            if abs(x - columns[i]) < abs(x - columns[best]):\n",
      "                best = i\n",
      "        return best\n",
      "\n",
      "    table = [[False] * len(columns) for row in range(0, 26)]\n",
      "\n",
      "    # Build table\n",
      "    # Find columns\n",
      "    for textline in textlines:\n",
      "        row = compute_row(textline)\n",
      "        if row != None:\n",
      "            col = compute_col(textline)\n",
      "            table[row][col] = textline['text'].strip()\n",
      "\n",
      "    process_achd_site(site, date, table)\n",
      "    \n",
      "def process_doc(doc, pages):\n",
      "    for (pageno, page) in enumerate(doc.getElementsByTagName('page')):\n",
      "        if not (pageno in pages):\n",
      "            continue\n",
      "        log('-----------Processing page %d' % pageno)\n",
      "        process_page(page)\n",
      "        sys.stdout.flush()\n",
      "\n",
      "\n",
      "\n",
      "def pdf_to_xml(pdf_path):\n",
      "    cache_file = pdf_path + '.cache.xml'\n",
      "    if enable_pdf_to_xml_cache and os.path.exists(cache_file):\n",
      "        print 'Using %s from cache' % (cache_file)\n",
      "        return open(cache_file).read()\n",
      "    else:\n",
      "        log('Converting %s to xml' % pdf_path)\n",
      "        start_time = time.time()\n",
      "        xml_content = subprocess.check_output([pdf2txt_path, '-t', 'xml', pdf_path])\n",
      "        print 'pdf %s length %d converted to xml length %d in %.1f seconds' % (pdf_path, os.stat(pdf_path).st_size, len(xml_content), time.time() - start_time)\n",
      "        if enable_pdf_to_xml_cache:\n",
      "            tmp_file = '%s.tmp.%d' % (cache_file, os.getpid())\n",
      "            open(tmp_file, 'w').write(xml_content)\n",
      "            os.rename(tmp_file, cache_file)\n",
      "            print 'xml written to %s' % cache_file\n",
      "        return xml_content\n",
      "    \n",
      "def process_pdf(pdf_path, pages=range(0, 1000)):\n",
      "    done_dir = 'upload-to-esdr'\n",
      "    try:\n",
      "        os.mkdir(done_dir)\n",
      "    except OSError:\n",
      "        pass\n",
      "    done_path = done_dir + '/' + os.path.basename(os.path.splitext(pdf_path)[0]) + '.successful-upload'\n",
      "    try:\n",
      "        last_done_path = sorted(glob.glob(done_dir + '/*.successful-upload'))[-1]\n",
      "        if not force_reprocess and os.path.exists(done_path):\n",
      "            return\n",
      "    \n",
      "    reset_log()\n",
      "    xml_content = pdf_to_xml(pdf_path)\n",
      "    if not enable_parse:\n",
      "        print 'Parse disabled'\n",
      "        return\n",
      "    log('Parsing and uploading %s' % pdf_path)\n",
      "    start_time = time.time()\n",
      "    doc = xml.dom.minidom.parseString(xml_content)\n",
      "    process_doc(doc, pages)\n",
      "    if enable_upload:\n",
      "        open(done_path + '.tmp', 'w').write(get_log())\n",
      "        os.rename(done_path + '.tmp', done_path)\n",
      "    print 'XML parsed and uploaded in %.1f seconds' % (time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "esdr = Esdr()\n",
      "esdr_product = esdr.get_product_by_name('ACHD')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#process_pdf('mirror/DailySummary-2014-11-30-00:35:02-0500.PDF')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_all():\n",
      "    lockfile = open('parse-achd.lockfile','w')\n",
      "    try:\n",
      "        fcntl.flock(lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
      "    except IOError:\n",
      "        print 'Instance of %s is already running.  Exiting.' % __file__\n",
      "        return\n",
      "    for pdf in sorted(glob.glob('mirror/*.PDF')):\n",
      "        process_pdf(pdf)\n",
      "\n",
      "process_all()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "timemachine1:/usr4/web/data.cmucreatelab.org/mirror-achd-air-quality-reports/mirror\n",
      "\n",
      "There are around 180 individual days\n",
      "Individual day takes around 2 minutes\n",
      "\n",
      "180 days * 2 min/day = 360 mins = 6 hrs\n",
      "\n",
      "Plus 500 days of historical reports\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}